# Papers List

This repository lists papers authored by **FocoosAI**.

## 2024

| Title | Venue | Code |
|:---|:---:|:---:|
| üìú [**PEM: Prototype-based Efficient MaskFormer for Image Segmentation**](https://openaccess.thecvf.com/content/CVPR2024/html/Cavagnero_PEM_Prototype-based_Efficient_MaskFormer_for_Image_Segmentation_CVPR_2024_paper.html) <br>Niccol√≤ Cavagnero, Gabriele Rosi, Claudia Cuttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli <br><br> *Prototype-based Efficient MaskFormer (PEM) is a transformer-based architecture for image segmentation that improves efficiency without sacrificing performance. It uses prototype-based cross-attention and a multi-scale feature pyramid network to reduce computation. PEM outperforms task-specific models while being more computationally efficient.* | CVPR 2024 | üåê <br> [Project Page](https://niccolocavagnero.github.io/PEM/) <br><br>[![GitHub stars](https://img.shields.io/github/stars/NiccoloCavagnero/PEM.svg?logo=github&label=Stars)](https://github.com/NiccoloCavagnero/PEM) |
| üìú [**The Revenge of BiSeNet: Efficient Multi-Task Image Segmentation**](https://openaccess.thecvf.com/content/CVPR2024W/ECV24/html/Rosi_The_Revenge_of_BiSeNet_Efficient_Multi-Task_Image_Segmentation_CVPRW_2024_paper.html) <br>Gabriele Rosi, Claudia Cuttano, Niccol√≤ Cavagnero, Giuseppe Averta, Fabio Cermelli <br><br> *BiSeNetFormer is a multi-task image segmentation architecture designed for efficiency and accuracy, supporting semantic and panoptic segmentation. It combines two-stream architectures with a transformer-based segmentation head, achieving high inference speeds and competitive accuracy on datasets like Cityscapes and ADE20K.* | CVPR 2024 (Workshop) | - |
| üìú [**What does CLIP know about peeling a banana?**](https://openaccess.thecvf.com/content/CVPR2024W/MAR/html/Cuttano_What_Does_CLIP_Know_About_Peeling_a_Banana_CVPRW_2024_paper.html) <br>Claudia Cuttano, Gabriele Rosi, Gabriele Trivigno, Giuseppe Averta <br><br> *AffordanceCLIP leverages pre-trained Vision-Language models like CLIP to improve affordance segmentation for robots, bypassing the need for costly annotations or predefined actions. It achieves competitive zero-shot performance, works with any action prompt, and requires minimal additional training, enabling scalable, flexible models.* | CVPR 2024 (Workshop) | - |
| üìú [**SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation**](https://arxiv.org/abs/2411.17646) <br>Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta <br><br> *SAMWISE is a Referring Video Object Segmentation (RVOS) method that overcomes limitations of previous models by enabling streaming processing while retaining context. Built on the Segment-Anything 2 (SAM2) model, it integrates natural language understanding and temporal modeling, achieving state-of-the-art performance with minimal overhead.* | üìù *Under submission* | [![GitHub stars](https://img.shields.io/github/stars/ClaudiaCuttano/SAMWISE.svg?logo=github&label=Stars)](https://github.com/ClaudiaCuttano/SAMWISE) |

---

Feel free to explore the papers and reach out for collaborations or inquiries!

