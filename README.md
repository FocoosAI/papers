# Papers List

This repository lists papers authored by **Focoos AI**.

## 2025 
| Title | Venue | Code |
|:---|:---:|:---:|
| üìú [**SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation**](https://arxiv.org/abs/2411.17646) <br>Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta <br><br> *SAMWISE is a Referring Video Object Segmentation (RVOS) method that overcomes limitations of previous models by enabling streaming processing while retaining context. Built on the Segment-Anything 2 (SAM2) model, it integrates natural language understanding and temporal modeling, achieving state-of-the-art performance with minimal overhead.* | **CVPR 2025** | [![GitHub stars](https://img.shields.io/github/stars/ClaudiaCuttano/SAMWISE.svg?logo=github&label=Stars)](https://github.com/ClaudiaCuttano/SAMWISE) |
| üìú [**To Match or Not to Match: Revisiting Image Matching for Reliable Visual Place Recognition**](https://arxiv.org/abs/2411.17646) <br> Davide Sferrazza, Gabriele Berton, Gabriele Trivigno, Carlo Masone <br><br> *This work demonstrates that modern retrieval systems often reach a saturation point where re-ranking may actually degrade performance due to the limitations of current VPR datasets. To address this, it proposes incorporating image matching as a verification step to evaluate retrieval confidence, showing that inlier counts serve as a reliable indicator for determining when re-ranking is truly beneficial.* | **CVPRW 2025** | [![GitHub stars](https://img.shields.io/github/stars/FarInHeight/To-Match-or-Not-to-Match.svg?logo=github&label=Stars)](https://github.com/FarInHeight/To-Match-or-Not-to-Match) |
| üìú [**Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation**](https://arxiv.org/abs/2505.06280) <br> Gabriele Rosi, Fabio Cermelli <br><br> *The paper introduces "Show or Tell", a benchmark comparing textual and visual prompts for semantic segmentation across 14 datasets in 7 domains. Results show text-based methods excel with common concepts but struggle with complex domains, while visual prompt methods achieve consistent results but vary based on prompt quality.* | **CVPRW 2025** | [![GitHub stars](https://img.shields.io/github/stars/FocoosAI/ShowOrTell.svg?logo=github&label=Stars)](https://github.com/FocoosAI/ShowOrTell) |
| üìú [AI Versus Nature: Navigating the Complex Interplay of Technology and the Environment](https://link.springer.com/book/10.1007/978-3-031-73514-1) <br>Barbara Caputo, Antonio Tavera, Fabio Cermelli, Giuseppe Roberto Marseglia<br><br> *The relationship between technology and the environment is complex, particularly with the advent of advanced artificial intelligence tools. While AI technologies consume significant energy and resources, they also offer solutions to environmental challenges. This chapter examines the dual impact of AI: increasing energy consumption and pollution, yet enabling more efficient and sustainable practices. Key applications include optimizing wind turbine placement, photovoltaic production, and energy consumption patterns. Additionally, the need for holistic energy assessments is emphasized, highlighting emerging efficient AI systems and advancements in public services like waste and water management.* | Springer Nature | - |



## 2024

| Title | Venue | Code |
|:---|:---:|:---:|
| üìú [**PEM: Prototype-based Efficient MaskFormer for Image Segmentation**](https://openaccess.thecvf.com/content/CVPR2024/html/Cavagnero_PEM_Prototype-based_Efficient_MaskFormer_for_Image_Segmentation_CVPR_2024_paper.html) <br>Niccol√≤ Cavagnero, Gabriele Rosi, Claudia Cuttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli <br><br> *Prototype-based Efficient MaskFormer (PEM) is a transformer-based architecture for image segmentation that improves efficiency without sacrificing performance. It uses prototype-based cross-attention and a multi-scale feature pyramid network to reduce computation. PEM outperforms task-specific models while being more computationally efficient.* | CVPR 2024 | üåê <br> [Project Page](https://niccolocavagnero.github.io/PEM/) <br><br>[![GitHub stars](https://img.shields.io/github/stars/NiccoloCavagnero/PEM.svg?logo=github&label=Stars)](https://github.com/NiccoloCavagnero/PEM) |
| üìú [**The Revenge of BiSeNet: Efficient Multi-Task Image Segmentation**](https://openaccess.thecvf.com/content/CVPR2024W/ECV24/html/Rosi_The_Revenge_of_BiSeNet_Efficient_Multi-Task_Image_Segmentation_CVPRW_2024_paper.html) <br>Gabriele Rosi, Claudia Cuttano, Niccol√≤ Cavagnero, Giuseppe Averta, Fabio Cermelli <br><br> *BiSeNetFormer is a multi-task image segmentation architecture designed for efficiency and accuracy, supporting semantic and panoptic segmentation. It combines two-stream architectures with a transformer-based segmentation head, achieving high inference speeds and competitive accuracy on datasets like Cityscapes and ADE20K.* | CVPR 2024 (Workshop) | - |
| üìú [**What does CLIP know about peeling a banana?**](https://openaccess.thecvf.com/content/CVPR2024W/MAR/html/Cuttano_What_Does_CLIP_Know_About_Peeling_a_Banana_CVPRW_2024_paper.html) <br>Claudia Cuttano, Gabriele Rosi, Gabriele Trivigno, Giuseppe Averta <br><br> *AffordanceCLIP leverages pre-trained Vision-Language models like CLIP to improve affordance segmentation for robots, bypassing the need for costly annotations or predefined actions. It achieves competitive zero-shot performance, works with any action prompt, and requires minimal additional training, enabling scalable, flexible models.* | CVPR 2024 (Workshop) | - |


---

Feel free to explore the papers and reach out for collaborations or inquiries!

